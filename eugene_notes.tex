\documentclass{article}
\usepackage{fullpage}
\usepackage{amsmath, amssymb}

\newcommand{\mL}{\mathcal{L}}
\newcommand{\ip}[2]{\left\langle #1, #2 \right\rangle}

\include{macros}

\begin{document}

\paragraph{Outline of paper}

\begin{itemize}

    \item  Introduction:
\begin{itemize}
        \item developmental systems drift
        \item justification of passage to the infinitesimal model
\end{itemize}

    \item Model:
\begin{itemize}
        \item heuristics and pictures from simulation below
        \item justification of passage to the infinitesimal model
        \item comparison to simulation: shape of the distribution; recombination load
    \end{itemize}

    \item Conclusion
\begin{itemize}
        \item limiting distribution: general sizes and how it depends on selection and segregation variance
        \item recombionation load
        \item speed of approach to optimum (?)
        \item influence of dimensionality
    \end{itemize}

\end{itemize}



\paragraph{General picture}
We want to describe the dynamics of a population that is evolving under stabilizing selection around not a single point but a manifold
of optimal phenotypes, using the infinitesimal model.
We think the right picture is that as the population size $N$ grows,
the dynamics of the measure become deterministic,
which flow quickly to within $1/N$ of the manifold,
and then we can follow only the mean as it diffuses as an OU-like process
near the manifold,
with drift and diffusion terms depending on the (deterministic) shape of the stable population measure
around that point.


\paragraph{Deterministic flow}
Let $X$ be a population of size $N$, recorded as the empirical distribution on phenotypes
\begin{align*}
    X = \frac{1}{N} \sum_{i=1}^N \delta_{x_i} .
\end{align*}
This evolves as a Moran process under the infinitesimal model
with death rates depending on state $x$.
Concretely, an individual whose phenotype is $x$
dies at rate $1 + \mu(x)$
and is replaced by a new individual of phenotype
\begin{align*}
  \frac{y + z}{2} + \xi ,
\end{align*}
where $y$ and $z$ are independent draws from $X$
and $\xi$ is an independent $N(0, \eta^2)$.
Let $\nu(X)$ denote the distribution of $\frac{y + z}{2} + \xi$.
This has generator
\begin{align*}
    \mL \Phi(X)
    &=
    \int \int \int \int
    \left( \Phi(X + \frac{1}{N} \delta_{(y+z)/2 + \xi} - \frac{1}{N} \delta_x) - \Phi(X) \right)
    (1 + \mu(x)) X(dx) X(dy) X(dz) e^{-(\xi/\eta)^2/2} d\xi \\
    &=
    \int \int
    \left( \Phi(X + \frac{1}{N} \delta_u - \frac{1}{N} \delta_x) - \Phi(X) \right)
    (1 + \mu(x)) X(dx) \nu(X)(du) .
\end{align*}

For a test function $\phi$, setting $\Phi(X) = \ip{\phi}{X}$
we get the first moment
\begin{align*}
    \mL \ip{\phi}{X}
    &=
    \frac{1}{N} \int \int \int \int
    \left( \phi \left( \frac{y+z}{2} + \xi \right)
         - \phi(x) \right)
    (1 + \mu(x)) X(dx) X(dy) X(dz) e^{-(\xi/\eta)^2/2} d\xi  \\
    &=
    \frac{1}{N} \ip{\phi}{\nu(X)}\ip{1+\mu}{X}
    - \frac{1}{N} \ip{\phi}{(1+\mu) X} .
\end{align*}
Note that $\ip{\phi}{\partial_{\nu(X)} X} = \ip{\phi}{\nu(X)}$,
so that this is a first derivative in the direction of
$\ip{1+\mu}{X} \nu(X) - (1+\mu) X$.

More generally, if $\Phi(X) = \prod_i \ip{\phi_i}{X}$ then
\begin{align*}
    \mL \Phi(X)
    &=
    \frac{1}{N} \sum_i \left( 
        \ip{1+\mu}{X} \ip{\phi_i}{\nu(X)}  - \ip{\phi_i}{(1+\mu)X}
    \right)
    \prod_{j \neq i} \ip{\phi_j}{X} 
    + O(1/N^2) .
\end{align*}
This implies that after rescaling time by $N$
and suitible other caveats,
the limiting process is a deterministic flow in the direction of 
$\nu(X) \ip{1+\mu}{X} - (1+\mu)X$.

\paragraph{Stable measures}
If $Z$ is a fixed point of this flow,
then $\nu(Z)\ip{1+\mu}{Z} = (1+\mu)Z$.
We assume that $\mu(x) = 0$ on a nice smooth ``optimal'' manifold,
and that it increases with distance to the manifold.
With $\eta = 0$,
any fixed point is either a point mass,
or possibly uniform on the optimal manifold if it is linear.
We think an argument for this is obtained SOMETHING LIKE THE FOLLOWING: 
if $Z$ is a fixed point
and $\phi$ is a test function that is convex and increasing with distance from the optimal manifold,
then $\ip{\phi}{\nu(Z)} < \ip{\phi}{Z} < \ip{\phi}{(1+\mu)Z}/\ip{1+\mu}{Z}$,
since averaging contracts $Z$ while reweighting by $1+\mu$ expands $Z$.

If $\eta$ is not zero, then we hope that for each point $z$ on the optimal manifold,
there exists a unique measure $Z(z)$ with mean $z$ that is a fixed point
of the flow.

Any measure $Z$ that is a fixed point has the property that
(the average of two independent draws plus segregation noise)
is equal in distribution to
(a $1+\mu$-weighted draw).
Concretely, with $A$, $B$, and $C$ independent draws from $Z$,
$$\begin{aligned}
    \E\left[ f\left( \frac{A+B}{2} + \xi \right) \right]
    &=
    \frac{\E\left[ f(C) (1+\mu(C)) \right]}{\E[1+\mu(C)]} .
\end{aligned}$$
Let
$$\begin{aligned}
    m_k = \E[A^k] = \ip{x^k}{X} ,
\end{aligned}$$
and note that since $\E[\xi^w] = \eta^w (w-1)!!$ for even $w$,
$$\begin{aligned}
    \ip{x^k}{\nu(X)}
    &=
    \sum_{u+v+w=k} \binom{k}{u,v,w} 2^{w-k} m_u m_v \eta^w (w-1)!!.
\end{aligned}$$

We can calculate some things about this measure
for the case that $\mu(x) = ax^2$ 
(or generally, if this is a Taylor expansion).
Then
$$\begin{aligned}
    \frac{\E\left[ C^k (1+\mu(C)) \right]}{\E[1+\mu(C)]} 
    &=
    \frac{ m_k + a m_{k+2} }{ 1 + a m_2 } .
\end{aligned}$$
This gives us an equation for $m_{k+2}$ in terms of lower-order terms.
With $k=2$ and assuming that $Z$ is symmetric,
$$\begin{aligned}
    \frac{m_2}{2} + \eta^2
    &=
    \frac{ m_2 + a m_4 }{ 1 + a m_2 },
\end{aligned}$$
and so 
$$\begin{aligned}
    m_4
    &= m_2 \left( \eta^2 + \frac{1}{2} m_2 \right) 
       + \frac{1}{a} \left( \eta^2 - \frac{1}{2} m_2 \right) .
\end{aligned}$$
We believe that as $a \to 0$ this does not blow up,
implying that $m_2 = 2 \eta^2 + O(a)$,
and that in fact approaches Gaussian, for which $m_4 = 3m_2^2$,
and so $\eta^2 - m_2/2 = a \eta^2 + O(a^2)$,
and hence
$$\begin{aligned}
    m_2 &= 2 \eta^2 ( 1 - a \eta^2 ) + O(a^2)  \\
    m_4 &= 4 \eta^4 + O(a) .
\end{aligned}$$

If we take $1+\mu(x) = e^{ax^2}$,
then there is a Gaussian solution,
as is common in quant gen.
(Write this down in arbitrary dimensions!)


\paragraph{Fitness and load}
Note that since reproduction is unweighted,
fitness is proportional to lifetime,
so the fitness function is $1/(1+\mu(x)$.
Since replacement occurs at rate $\ip{1+\mu}{X}$,
the mean number of offspring per individual of type $x$
in population $X$ is $\ip{1+\mu}{X} / (1+\mu(x))$.

Since $\nu(X)$ is the offspring distribution,
and we know that $\nu(X) = (1+\mu) X / \ip{1+\mu}{X}$,
we can compute mean fitness of offspring as
$$\begin{aligned}
    \ip{\frac{1}{1+\mu}}{\nu(X)}
    &=
    \frac{\ip{\frac{1}{1+\mu}}{(1+\mu)X}}{\ip{1+\mu}{X}} \\
    &=
    \frac{1}{\ip{1+\mu}{X}} .
\end{aligned}$$

Take $\mu(x) = ax^2$.
The mean fitness in the population is then
$$\begin{aligned}
    \ip{1/(1+\mu)}{X}
    &=
    \ip{1 - ax^2 + a^2 x^4}{X} + O(a^3) \\
    &=
    1 - a m_2 + a^2 m_4 + O(a^3) \\
    &=
    1 - 2 a \eta^2 + 6 a^2 \eta^4 + O(a^3) ,
\end{aligned}$$
and the mean fitness among offspring is
$$\begin{aligned}
    \frac{1}{1+am_2} 
    &= 1 - 2 a \eta^2 + 2 a^2 \eta^4 + O(a^3),
\end{aligned}$$
so the difference in mean fitness between parents and offspring
(maybe this is called recombination load?)
is $4 a^2 \eta^4$.

Here is the same thing calculated the other way,
but something is wrong:
The mean fitness of offspring is
$$\begin{aligned}
    \ip{1/(1+\mu)}{\nu(X)}
    &=
    \ip{1 - ax^2 + a^2 x^4}{\nu(X)} + O(a^3) \\
    &=
    1 - a \left( \frac{m_2}{2} + \eta^2 \right) 
        + a^2 \left( 2 m_4 + 6 m_2^2 + 12 m_2 \eta^2 + 3 \eta^4\right) + O(a^3) \\
    &=
    1 - a \left( 2 \eta^2 - a \eta^4 \right)
        + a^2 \left( 8 \eta^4 + 24 \eta^4 + 24 \eta^4 + 3 \eta^4\right) + O(a^3) \\
    &=
    1 - 2 a \eta^2 + 60 a^2 \eta^4 + O(a^3) .
\end{aligned}$$
The difference here is
is $54 a^2 \eta^4 + O(a^3)$.
(something is wrong here because mean fitness in offspring should be less than in parents).



\paragraph{Diffusion of the mean}
Since we work with a Moran model,
the mean evolves simply by jumping at rate $\ip{1+\mu}{X}$,
and at each jump, adding $w/N$ and subtracting $\tilde x/N$,
where $w$ is drawn from $\nu(X)$ and $\tilde x$ is drawn from 
$(1+\mu)X/\ip{1+\mu}{X}$.
This implies that if the mean is currently $\bar X$
and the population distribution is $X$,
then with $\phi_{id}(x) = x$,
if
\begin{align*}
    m(X) = N \left( \ip{\phi_{id}}{\nu(X)}\ip{1+\mu}{X}
    -
    \ip{\phi_{id}}{(1+\mu)X} \right)
\end{align*}
is of order 1,
then
the $\bar X$ run on a time scale of $N^2$
should behave as a diffusion with drift $m(X)$
and variance equal to the variance of $w$ plus the variance of $\tilde x$, which is
\begin{align*}
    \frac{1}{2} \int (x-\bar X)^2 X(dx) + \eta^2
    + \frac{ \int x^2 (1+\mu(x)) X(dx)  
    - \left(\int x (1+\mu(x)) X(dx)\right)^2 }{ \ip{1+\mu}{X} } .
\end{align*}

\paragraph{Joint process}
By writing the joint dynamics of the mean and the measure realtive to the mean,
we hope to show that the picture above makes sense,
that in the limit we can take the measure as deterministic given the mean.

\paragraph{Weak selection}
It may be nice to consider weak selection,
in which case perhaps the limiting measures are all Gaussian.


\end{document}
